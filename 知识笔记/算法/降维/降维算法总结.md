# [降维算法总结](https://blog.csdn.net/qq_35719435/article/details/83721345)

[toc]

### [总结网址](https://blog.csdn.net/ma416539432/article/details/53286028?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&utm_relevant_index=2)
1. 降维算法思维导图：
![avatar](索引/%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png)

#### 一. 基本概念和作用  

**1. 维度目的:** 用来进行特征选择和特征提取。
   + 特征选择：选择重要的特征子集，删除其余特征。
   + 特征提取：由原始特征形成较少的新特征。

**[2. 决策树:](https://blog.csdn.net/jiaoyangwm/article/details/79525237?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164998693116780255284467%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=164998693116780255284467&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-79525237.142^v8^pc_search_result_cache,157^v4^control&utm_term=%E5%86%B3%E7%AD%96%E6%A0%91&spm=1018.2226.3001.4187)**
+ 决策树学习的目标：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。

+ 决策树学习的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。

+ 决策树学习的损失函数：正则化的极大似然函数

+ 决策树学习的测试：最小化损失函数

+ 决策树学习的目标：在损失函数的意义下，选择最优决策树的问题。

+ 决策树原理和问答猜测结果游戏相似，根据一系列数据，然后给出游戏的答案。

+ k-近邻算法可以完成很多分类任务，但是其最大的缺点是无法给出数据的内在含义，决策树的优势在于数据形式非常容易理解。

(1)决策树的构建：
+ 决策树学习的算法通常是一个递归地选择<font color=red>最优特征</font>，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。
+ 构建步骤：
    a. 开始：构建根节点，将所有训练数据都放在根节点，选择一个最优特征，按着这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。

    b. 如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去。

    c. 如果还有子集不能够被正确的分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点，如果递归进行，直至所有训练数据子集被基本正确的分类，或者没有合适的特征为止。

    d. 每个子集都被分到叶节点上，即都有了明确的类，这样就生成了一颗决策树。



**3. 降维的作用:**
   + 降低时空复杂度。
   + 节省了提取不必要特征的时间。
   + 去掉数据集中夹杂的噪音。
   + 较简单的模型在小数据集上有更强的 **<u>鲁棒性</u>** *(鲁棒是Robust的音译，也就是健壮和强壮的意思。它也是在异常和危险情况下系统生存的能力。)*
   + 当数据能有较少的特征进行解释时，我们可以更好的解释数据，使得我们可以提取知识。
   + 实现数据可视化。


### 线性降维
[**<font color=green>1. PCA</font>**](https://blog.csdn.net/program_developer/article/details/80632779?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165087125316781435491727%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=165087125316781435491727&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-80632779.142^v9^pc_search_result_control_group,157^v4^control&utm_term=PCA&spm=1018.2226.3001.4187)
+ 在进行pca分析之前，我们先对数据进行标准化处理。让数据处于 **[正态分布](https://blog.csdn.net/weixin_43899514/article/details/115215980)** 的形式下更好
+ 无监督学习
+ 我的理解：PCA是找一个维度让数据之间的方差达到最大这个维度对每个特征有个打分，特征是根据样本情况进行评估，然后得到了第一个最大方差也是PC1对样本的解释程度最大的主成分，PC2在1的基础上继续寻找方差最大的维度，在根据这个维度对特征进行评估，以此类推。同时PCA根据SVD分解(来求得了奇异值，对数据进行压缩)，这可能是他维度得由来。
+ PCA缺点：
    + PCA是线性降维，有时候数据之间的非线性关系是很重要的，这时候我们用pca会得到很差的结果。所有接下来我们引入核方法的pca。
    + 主成分分析法只在样本点服从高斯分布的时候比较有效。
    + 特征根的大小决定了我们感兴趣信息的多少。即小特征根往往代表了噪声，但实际上，向小一点的特征根方向投影也有可能包括我们感兴趣的数据；
    + 难于解释结果。例如在建立线性回归模型（Linear Regression Model）分析因变量（response）和第一个主成份的关系时，我们得到的回归系数（Coefficiency）不是某一个自变量（covariate）的贡献，而是对所有自变量的某个线性组合（Linear Combination）的贡献。
    

[**<font color=green>2. LDA(线性判别分析)</font>**](https://baijiahao.baidu.com/s?id=1698067394400927659&wfr=spider&for=pc)
+ LDA，是分类算法中的一种。LDA通过对历史数据进行投影，以保证投影后同一类别的数据尽量靠近，不同类别的数据尽量分开。并生成线性判别模型对新生成的数据进行分离和预测。
+ 有监督学习
+ 主要依靠平均值来分类。

[PCA与LDA对比](https://www.cnblogs.com/jerrylead/archive/2011/04/21/2024389.html)

### 流行学习(非线性降维)
[**<font color=green>1.t-SNE降维的由来</font>**](http://bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/)